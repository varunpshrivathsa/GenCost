Step1
ğŸ§© Stage 1 â€“ Step 1 : Foundation & Data Layer (Completed)
ğŸ¯ Goal

Lay the groundwork that lets the project measure cost, latency, and output quality for any LLM call â€” reproducibly and in a unified format.
This stage isnâ€™t about calling real APIs yet; itâ€™s about establishing a clean, measurable backbone that later agents, policies, and dashboards will rely on.

âš™ï¸ 1. Project Initialization

Repository: llm-cost-optimizer (or GenCost) created on GitHub.

Environment: Local virtual environment (.venv) + reproducible dependency list in requirements.txt.

Folder layout:

src/
  core/        â†’ foundation modules (config, cost tracker, DB)
  benchmarks/  â†’ scripts to run test batches (next step)
  utils/       â†’ text helpers (later)
notebooks/     â†’ for visual analysis


âœ… Outcome: reproducible workspace with version control, virtualenv, and dependency isolation.

ğŸ§© 2. Configuration & Pricing Registry
src/core/config.py

Centralizes environment variables and model-pricing data.

Loads .env via python-dotenv.

Defines ModelPricing dataclass to represent rates cleanly.

Exposes helper load_pricing() â†’ returns a dict mapping "provider:model" â†’ pricing info.

src/core/pricing.json

Editable JSON file that stores per-model input/output token prices (USD / 1 M tokens).
Example:

"openai:gpt-4o": { "input_per_million": 5.00, "output_per_million": 15.00 }


âœ… Outcome: pricing lives outside the code, so cost updates need no code changes.

ğŸ§® 3. Metrics Database (Persistence Layer)
src/core/metrics_db.py

Implements a light ORM schema using SQLAlchemy with two normalized tables:

Table	Purpose	Key Columns
requests	metadata about each LLM call	request_id, ts, provider, model, latency_ms, status
responses	outcome & metrics of that call	request_id, response, usage_tokens_in/out, cost_usd, quality_proxy

The function init_db() creates both tables automatically in SQLite (default llmopt_stage1.sqlite).

Later, this can be swapped to PostgreSQL by changing LLMOPT_DB_URL in .env.

âœ… Outcome: a durable metrics store ready for logging every LLM transaction.

ğŸ’µ 4. Cost Computation Engine
src/core/cost_tracker.py

Encapsulates cost logic in a small reusable class:

CostTracker().compute_cost_usd("openai:gpt-4o", tokens_in=1000, tokens_out=2000)


Pulls live pricing from pricing.json.

Calculates cost as

\text{cost} = \frac{\text{tokens_in}}{10^6}Â·\text{input\_rate} + \frac{\text{tokens_out}}{10^6}Â·\text{output\_rate}

Returns cost rounded to 6 decimals.

âœ… Outcome: cost accounting is centralized, testable, and provider-agnostic.

ğŸ§  5. Validation & Smoke Tests

You successfully ran:

python -c "from src.core.metrics_db import init_db; init_db()"


â†’ Created the DB schema âœ…
and

python -c "from src.core.cost_tracker import CostTracker; print(CostTracker().compute_cost_usd('openai:gpt-4o',1000,2000))"


â†’ Returned 0.035 USD âœ…

These confirm both the database and pricing pipeline are operational.

ğŸ§° 6. What You Have Now
Layer	Component	Responsibility
Config	.env + config.py	Manage DB URL and paths
Pricing	pricing.json	Store updatable rates
Persistence	metrics_db.py	Log requests/responses
Accounting	cost_tracker.py	Compute costs consistently

All these modules are provider-neutral, stateless, and unit-testable â€” ideal for reuse in later RL, API, and dashboard stages.

ğŸ 7. Why It Matters

Establishes data fidelity: every experiment, model call, or agent decision is traceable.

Enables comparability: same schema across providers â†’ identical analytics downstream.

Provides auditability: cost, latency, and quality can be reproduced for any request ID.

Forms the foundation for upcoming layers:
 
Step 2 â†’ unified LLM wrapper + mock provider
âœ… What you just proved
Layer	What happened	Verified
UnifiedLLM	Normalized your input prompt and generated a UUID for tracking	âœ…
MockProvider	Simulated latency and output tokens (9 in â†’ 9 out)	âœ…
CostTracker	Computed per-call cost using pricing.json (0 USD since mock model not priced)	âœ…
Quality Proxy	Estimated diversity metric (â‰ˆ 0.61) using type-token ratio + response length	âœ…
Metrics DB	Persisted entries to both requests and responses tables in llmopt_stage1.sqlite	âœ…

Step3 
ğŸ§© Stage 1 â€“ Step 3 : Baseline Benchmark Runner âœ…
ğŸ¯ Goal

To collect measurable baseline metrics (cost, latency, and quality proxy) across multiple LLM models, using your unified wrapper from Step 2.
This stage transforms your cost-tracking system into a quantitative benchmarking tool.

âš™ï¸ What We Built
1ï¸âƒ£ src/benchmarks/prompts.csv

A small dataset of 5 prompts (QA, reasoning, coding, long-form, extraction).
Purpose â†’ simulate realistic query diversity so latency and quality can be compared meaningfully across model tiers.

2ï¸âƒ£ src/benchmarks/run_baseline.py

A benchmarking script that:

Imports the unified wrapper (UnifiedLLM) and initializes it with the MockProvider.

Randomly selects prompts Ã— models (here small-latest, large-latest).

Runs N (e.g., 50) requests while:

Measuring latency (ms) per request.

Recording simulated token counts.

Computing per-call cost via CostTracker.

Estimating â€œquality proxyâ€ (type-token ratio + length).

Stores everything in:

SQLite (requests + responses tables).

CSV output â†’ baseline_results.csv.

Aggregates statistics per model:

median (p50) latency

p95 latency

average cost

average quality proxy

Generates two plots using matplotlib:

baseline_cost_vs_latency.png â†’ scatter chart.

baseline_quality.png â†’ bar chart.
Step1
🧩 Stage 1 – Step 1 : Foundation & Data Layer (Completed)
🎯 Goal

Lay the groundwork that lets the project measure cost, latency, and output quality for any LLM call — reproducibly and in a unified format.
This stage isn’t about calling real APIs yet; it’s about establishing a clean, measurable backbone that later agents, policies, and dashboards will rely on.

⚙️ 1. Project Initialization

Repository: llm-cost-optimizer (or GenCost) created on GitHub.

Environment: Local virtual environment (.venv) + reproducible dependency list in requirements.txt.

Folder layout:

src/
  core/        → foundation modules (config, cost tracker, DB)
  benchmarks/  → scripts to run test batches (next step)
  utils/       → text helpers (later)
notebooks/     → for visual analysis


✅ Outcome: reproducible workspace with version control, virtualenv, and dependency isolation.

🧩 2. Configuration & Pricing Registry
src/core/config.py

Centralizes environment variables and model-pricing data.

Loads .env via python-dotenv.

Defines ModelPricing dataclass to represent rates cleanly.

Exposes helper load_pricing() → returns a dict mapping "provider:model" → pricing info.

src/core/pricing.json

Editable JSON file that stores per-model input/output token prices (USD / 1 M tokens).
Example:

"openai:gpt-4o": { "input_per_million": 5.00, "output_per_million": 15.00 }


✅ Outcome: pricing lives outside the code, so cost updates need no code changes.

🧮 3. Metrics Database (Persistence Layer)
src/core/metrics_db.py

Implements a light ORM schema using SQLAlchemy with two normalized tables:

Table	Purpose	Key Columns
requests	metadata about each LLM call	request_id, ts, provider, model, latency_ms, status
responses	outcome & metrics of that call	request_id, response, usage_tokens_in/out, cost_usd, quality_proxy

The function init_db() creates both tables automatically in SQLite (default llmopt_stage1.sqlite).

Later, this can be swapped to PostgreSQL by changing LLMOPT_DB_URL in .env.

✅ Outcome: a durable metrics store ready for logging every LLM transaction.

💵 4. Cost Computation Engine
src/core/cost_tracker.py

Encapsulates cost logic in a small reusable class:

CostTracker().compute_cost_usd("openai:gpt-4o", tokens_in=1000, tokens_out=2000)


Pulls live pricing from pricing.json.

Calculates cost as

\text{cost} = \frac{\text{tokens_in}}{10^6}·\text{input\_rate} + \frac{\text{tokens_out}}{10^6}·\text{output\_rate}

Returns cost rounded to 6 decimals.

✅ Outcome: cost accounting is centralized, testable, and provider-agnostic.

🧠 5. Validation & Smoke Tests

You successfully ran:

python -c "from src.core.metrics_db import init_db; init_db()"


→ Created the DB schema ✅
and

python -c "from src.core.cost_tracker import CostTracker; print(CostTracker().compute_cost_usd('openai:gpt-4o',1000,2000))"


→ Returned 0.035 USD ✅

These confirm both the database and pricing pipeline are operational.

🧰 6. What You Have Now
Layer	Component	Responsibility
Config	.env + config.py	Manage DB URL and paths
Pricing	pricing.json	Store updatable rates
Persistence	metrics_db.py	Log requests/responses
Accounting	cost_tracker.py	Compute costs consistently

All these modules are provider-neutral, stateless, and unit-testable — ideal for reuse in later RL, API, and dashboard stages.

🏁 7. Why It Matters

Establishes data fidelity: every experiment, model call, or agent decision is traceable.

Enables comparability: same schema across providers → identical analytics downstream.

Provides auditability: cost, latency, and quality can be reproduced for any request ID.

Forms the foundation for upcoming layers:
 
Step 2 → unified LLM wrapper + mock provider
✅ What you just proved
Layer	What happened	Verified
UnifiedLLM	Normalized your input prompt and generated a UUID for tracking	✅
MockProvider	Simulated latency and output tokens (9 in → 9 out)	✅
CostTracker	Computed per-call cost using pricing.json (0 USD since mock model not priced)	✅
Quality Proxy	Estimated diversity metric (≈ 0.61) using type-token ratio + response length	✅
Metrics DB	Persisted entries to both requests and responses tables in llmopt_stage1.sqlite	✅

Step3 
🧩 Stage 1 – Step 3 : Baseline Benchmark Runner ✅
🎯 Goal

To collect measurable baseline metrics (cost, latency, and quality proxy) across multiple LLM models, using your unified wrapper from Step 2.
This stage transforms your cost-tracking system into a quantitative benchmarking tool.

⚙️ What We Built
1️⃣ src/benchmarks/prompts.csv

A small dataset of 5 prompts (QA, reasoning, coding, long-form, extraction).
Purpose → simulate realistic query diversity so latency and quality can be compared meaningfully across model tiers.

2️⃣ src/benchmarks/run_baseline.py

A benchmarking script that:

Imports the unified wrapper (UnifiedLLM) and initializes it with the MockProvider.

Randomly selects prompts × models (here small-latest, large-latest).

Runs N (e.g., 50) requests while:

Measuring latency (ms) per request.

Recording simulated token counts.

Computing per-call cost via CostTracker.

Estimating “quality proxy” (type-token ratio + length).

Stores everything in:

SQLite (requests + responses tables).

CSV output → baseline_results.csv.

Aggregates statistics per model:

median (p50) latency

p95 latency

average cost

average quality proxy

Generates two plots using matplotlib:

baseline_cost_vs_latency.png → scatter chart.

baseline_quality.png → bar chart.